{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60c2f0ac-9a27-4ca1-ba82-8e2827952c38",
   "metadata": {},
   "source": [
    "# FINAL PROJECT REPORT - DSCI 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3901614a-4fe0-431e-90d3-0971c439f224",
   "metadata": {},
   "source": [
    "## Predicting number of played hours on a MineCraft server based on player experience level, gender, and age. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13725cc5-c17f-4c92-9083-aa77167b31bf",
   "metadata": {},
   "source": [
    "##### Yuxuan Zhang, Niloo Nasiri Faskhodi, Thavishi Pratap, Govind Venkat Narayanan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace36ec4-28be-46aa-946d-0c655031ef91",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Introduction:\n",
    "Video games generate a lot of information about how players behave. The UBC Computer Science research group led by Frank Wood is running a special Minecraft research server to study these behaviours. Every time a player joins the server, their actions and play sessions are recorded. In this project, we wants to use this data to understand players better and also to improve how they can run the server in a more efficient way.\n",
    "\n",
    "Understanding which players tend to generate the most gameplay data is especially important, as they can bring us more information so we can figure out way to improve. The research has shown that playtime across different groups. International studies have reported differences in the played hours across genders. Some finding that men tend to spend more time gaming than women on average (Rehbein et al., 2016). Other work shows that gaming participation is also different across age groups: roughly half of both men and women report playing some form of video game. Young adults are the most active players, while many adults aged 50–64 and even 65+ also play regularly (Bunz et al., 2020). These researches indicates that age and gender may meaningfully decide on gaming behaviour.\n",
    "\n",
    "In our project, we research about whether similar patterns appear in the Minecraft research server data. Specifically, we aim to identify which “kinds” of players are most likely to contribute a large amount of gameplay data so that efforts can be spent on those groups more effectively. Players who play longer produce richer behavioural data, which is valuable for future research.\n",
    "\n",
    "Our project focuses on the general question:\n",
    "\n",
    "**Which \"kinds\" of players are most likely to contribute a large amount of data so that we can target those players in our recruiting efforts.**\n",
    "\n",
    "This question is very meaningful because players who play longer create more data, which is helpful for the research group. If we can predict which players will have longer playtime, the team can focus their efforts on those types of players and generate more useful data.\n",
    "\n",
    "The specific question that we're hoping to answer is as follows:\n",
    "\n",
    "**Can age, gender, and experience level predict the total number of hours a player plays on the MineCraft server using the players dataset?**\n",
    "\n",
    "To answer this question, we used the dataset: players.csv.\n",
    "\n",
    "The players dataset contains 196 unique players. Each row represents one player and includes their basic information and their overall gameplay history. The dataset has 7 variables: \n",
    "\n",
    "|Variable Name | Type | Description |\n",
    "|--------------| -----| ------------|\n",
    "|experience | categorical (factor) | How familiar the player is with Minecraft (“Beginner”, “Intermediate”, “Expert”, \"Pro\", \"Regular\")|\n",
    "|subscribe | categorical (logical) | Whether or not the player has a game subscription (subscribed= TRUE)|\n",
    "|hashedEmail | String | A hashed version of the player's email address. Not useful for modelling. An ID for each player|\n",
    "|played_hours | numeric (continuous) | TThe total number of hours the player has spent on the server across all sessions. This is the **response variable**|\n",
    "|name| String/Categorical | Player's chosen name on game server. Not useful for modelling|\n",
    "|gender| Categorical (factor) | Self reported gender category of the players |\n",
    "|age| Numeric (integer) | Player's age in years (2 missing values)|\n",
    "\n",
    "\n",
    "In this report, we will show how we tidied and wrangled the data, built and choose our model, and analyzed the results to understand what types of players tend to play more on the server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2713f7-a3f3-4b01-a7f6-c69e8ae0a79d",
   "metadata": {},
   "source": [
    "## Methods\n",
    "#### 1. Tidying and wrangling the data\n",
    "\n",
    "First, we imported all necessary libraries necessary for data analysis, and read our data from a URL that was linked to our GitHub repository.\n",
    "\n",
    "\n",
    "Then, we tidied the data so we could use it for our data analysis when building prediction models for the data. We ensured that our final data table satisfied the requirements of tidy data: each row is a single observation, each column is a single variable, and each value is a single cell. We also ensured that the final data set only included the variables we wanted to look at when we wanted to build our model. This included removing variables/columns such as `hashedEmail`, `name`, and `subscribe`.\n",
    "\n",
    "Afterwards, we made visualizations showcasing the relationship between each of our independent variables (`Age`, `gender`, and `experience`) with our dependent variable (`played_hours`) to better understand whether there even was a detectable relationship between the variables we would be creating a regression model with.\n",
    "\n",
    "#### 2. Building regression models for the data\n",
    "\n",
    "After gaining a better understanding of the relationships between our variables and tidying our data, we were ready to create a prediction model. Since we're planning to predict a numerical value, we chose to build a regression model rather than a classification model. Prior to starting our data analysis here, we were unsure as to which model would be better: KNN or linear regression.\n",
    "\n",
    "To determine which model was better, we have to consider the relationship between some of the variables. Since we have three predictor variables, our pattern will most likely not be the most linear, so using a KNN regression may be better because it can fit more complex patterns. Moreover, this flexibility in KNN regression may be helpful because our multivariable regression will most likely be non-linear. Based on this logic, using a KNN regression instead of a linear regression may be advantageous as it can result in a smaller RMSPE (root mean square prediction error) value. A smaller RMSPE value would indicate that the model made fewer errors in the sense that the model's predictions were closer in distance to the real values.\n",
    "\n",
    "First, we set a seed to ensure that our data is reproducible and that R splits the data the same way randomly every time our code is run. Then, we decided to split our tidy data into a training and testing set. We would first train and tune our models with our training set and then apply the model on the testing set to evaluate its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbd73d3-1653-488e-a287-a5f6261693a4",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0bdaab-b2d4-4043-a2c8-c21fdd9bf695",
   "metadata": {},
   "source": [
    "First, we import all necessary libraries for our data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2070bb-eb63-4a42-939e-ee4e2515e465",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the library\n",
    "library(tidyverse)\n",
    "library(repr)\n",
    "library(tidymodels)\n",
    "library(cowplot)\n",
    "library(ggplot2)\n",
    "library(RColorBrewer)\n",
    "#import glue library to connect variables with characters in a print statement\n",
    "library(glue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b21554-bb2e-4d40-b403-c381e8b39292",
   "metadata": {},
   "source": [
    "Now, we load our data using a URL from our GitHub repository, so it's accessible for anyone to load. We use `read_csv` to read the non-wrangled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42fcce5-557b-4adc-b041-de4fd55057eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "players_data <- read_csv(\"https://raw.githubusercontent.com/thaviship/dsci-group-project/refs/heads/main/players.csv\")\n",
    "head(players_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403327cd-8560-42f7-ba8c-52979e1ab051",
   "metadata": {},
   "source": [
    "*figure 1*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db8b17a-ea2e-4ac6-9c7e-0102a109679c",
   "metadata": {},
   "source": [
    "To tidy our data, we will only select our necessary variables to answer our question: `experience`, `gender`, `Age`, and `played_hours`. We decided to remove players who had 0 hours of gameplay as they would not contribute to the data whatsoever. All NA values associated with `Age` were also removed to ensure that all data points we have could be used for our model building later. We also created a standardized scale with the `played_hours` using logarithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4547cf9e-14f3-44d5-a9b0-c0c7bca37bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tidy data\n",
    "players_tidy <- players_data |> \n",
    "\n",
    "# Select out the hashedEmail and name column vectors as they are identifiers and contain no predictive information.\n",
    "                select(-hashedEmail, -name, -subscribe) |> \n",
    "\n",
    "#Convert subscribe, experience, and gender character variables to factor variables so that R can treat them as categorical variables.\n",
    "  #mutate(subscribe_fct=as_factor(subscribe)) |> \n",
    "  mutate(experience_fct=as_factor(experience)) |>\n",
    "  mutate(gender_fct=as_factor(gender)) |>\n",
    "\n",
    "#Filter out the played_hours variable and select only those who have >0 gameplay hours. Zero-hour players are non-contributors and do not contribute to the goal of identifying high-engagement users.\n",
    "                filter(played_hours>0) |> \n",
    "\n",
    "#Remove rows with missing Age, as leaving NA values leads to loss of rows silently and inconsistencies in model prediction.\n",
    "         filter(!is.na(Age)) |>   \n",
    "\n",
    "#To deal with extreme outliers in played_hours, transform highly skewed values into log as it compresses the large values, which makes the distribution closer to normal and stabilizes variance. \n",
    "         mutate(played_hours_log = log(played_hours + 1))\n",
    "               \n",
    "\n",
    "#remove non fct columns to prevent multicollinearlity when performing multivariable linear regression and to ease visualisation of the dataset.\n",
    "\n",
    "players_clean<- players_tidy |> drop_na(played_hours_log, experience_fct, gender_fct, Age) |> select(experience_fct, gender_fct, Age, played_hours_log)\n",
    "\n",
    "head(players_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb0697e-f6c0-44cf-a451-8b920ea3c256",
   "metadata": {},
   "source": [
    "*figure 2*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a538fae-e029-488a-9c07-1621b77e51a1",
   "metadata": {},
   "source": [
    "Below, the total number of hours played is graphed against the experience level of the players. The sum of hours played of each experience category was taken to produce the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da35a6f-d00a-473e-b330-255b1eef0acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_plot_tidy_players <- players_clean |>\n",
    "  group_by(experience_fct) |>\n",
    "  summarise(total_hours = sum(played_hours_log), .groups = \"drop\")\n",
    "bar_plot_tidy_players\n",
    "\n",
    "bar_plot_experience_vs_totalhrs <- bar_plot_tidy_players |> \n",
    "  ggplot(aes(x = fct_reorder(experience_fct, total_hours, .desc=TRUE),\n",
    "             y = total_hours,\n",
    "             fill = experience_fct)) +\n",
    "  geom_bar(stat = \"identity\") +\n",
    "  labs(x = \"Experience Level\", \n",
    "       y = \"Total Hours Played\", \n",
    "       fill = \"Experience Level\",title = \"Total Hours Played by Experience Level\")\n",
    "\n",
    "bar_plot_experience_vs_totalhrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3b09eb-c99f-47ec-92d5-9164065eea06",
   "metadata": {},
   "source": [
    "*figure 3*: The values for total played hours haven't been converted to percentages here, as the raw values show how much each group actually contributes and highlights the real difference between the self-reported experience level of the players and the total hours played. The biggest share of gameplay hours comes from mid-level players that fall into the Amateur and Regular categories. True novices like Beginners and highly skilled players like Pros contribute less to gameplay hours. Hence, these observations suggest that the core contributors to gameplay hours are mid-skilled players (Amateurs and Regulars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fb139c-df64-4d99-9085-80d5e2986e66",
   "metadata": {},
   "source": [
    "Next, we wanted to see the relationship between the age of players alongside their experience level and the quantity of these groups of people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cf5416-6295-44b7-a81e-9d1124a8049b",
   "metadata": {},
   "outputs": [],
   "source": [
    "players_age_hours_hist <- players_clean |> \n",
    "   ggplot(aes(x = Age, fill = experience_fct)) +\n",
    "  geom_histogram(binwidth = 2,color=\"black\", alpha=0.7) +\n",
    "  labs(\n",
    "    title = \"Distribution of Player Ages by Subscription Status\",\n",
    "    x = \"Age (years)\",\n",
    "    y = \"Total number of players per age category\",\n",
    "    fill = \"Experience Level\"\n",
    "  ) +\n",
    "  theme_minimal(base_size = 14) \n",
    "  \n",
    "players_age_hours_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea268399-9e20-4b38-8626-3c6968221dee",
   "metadata": {},
   "source": [
    "*figure 4*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15919e9a-3480-4f99-9702-eb0bda8e3db1",
   "metadata": {},
   "source": [
    "Below, the sum of played hours based on gender was calculated, which was followed by finding the distribution of played hours depending on the gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7633526e-2702-4ad4-8c2f-0c81eb3f7d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    " plot_data <- players_clean |>\n",
    "  group_by(gender_fct) |> \n",
    "  summarise(total_hours = sum(played_hours_log), .groups = \"drop\") |>\n",
    "  mutate(percentage = total_hours / sum(total_hours))\n",
    "\n",
    "experience_gender_percent_plot_fct <- ggplot(plot_data,\n",
    "                                             aes(x = gender_fct,\n",
    "                                                 y = percentage, fill=gender_fct)) +\n",
    "  geom_bar(stat = \"identity\", colour = \"black\") +\n",
    "  scale_y_continuous(labels = scales::percent_format()) +\n",
    "  labs(\n",
    "    x = \"Gender\",\n",
    "    y = \"Percentage of Total Played Hours\",\n",
    "    title = \"Distribution of Played Hours by Gender\",\n",
    "  fill=\"Gender\") +\n",
    "  theme_minimal()\n",
    "\n",
    "experience_gender_percent_plot_fct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6749e8-9665-4997-b7d3-65efc3e2f6bb",
   "metadata": {},
   "source": [
    "*figure 5*: The y value of this plot has been converted to a percentage, as it represents the relative contributions of each gender, given that it prevents a single heavy gamer from making a gender group look bigger. Hence, it clearly depicts that male players account for nearly 60% of the total hours logged, and female players contribute to roughly 25% followed by the other 4 categories which have minor contributions to gameplay hours. This suggests that gender is associated with overall gameplay hours, with the male category driving the majority of the hours.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad8ca0d-5152-4973-9691-0f23287aa11a",
   "metadata": {},
   "source": [
    "### Making a prediction model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffb7478-fc91-4863-be4b-d90e4be13a65",
   "metadata": {},
   "source": [
    "#### Predicting played hours based on experience level, age, and gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1d61d1-84a3-4479-8126-6b9ebc9f0ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.matrix.max.rows = 6)\n",
    "players_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6258966a-48ef-40d3-8fb9-b0e33c5783fa",
   "metadata": {},
   "source": [
    "*figure 6*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d5032b-68d4-4bc8-97f3-2b20d2849110",
   "metadata": {},
   "source": [
    "### Applying a KNN regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51449774-6585-4246-a20d-9d79208dc8fa",
   "metadata": {},
   "source": [
    "First, we set a seed and split our data. We chose to split/shuffle our data in a way such that 75% of it would be randomly set to be our training set, whereas the remaining 25% would be placed into our testing set. After training our classifier with our training set, we will apply it on the testing set to see if it is effective in making correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e264e711-0adf-4797-8887-c14a6a73b549",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting the seed\n",
    "set.seed(1234)\n",
    "\n",
    "#splitting the data into a training and testing set; a 75% proportion ratio will be used for the training data\n",
    "players_split <- initial_split(players_clean, prop = 0.75, strata = played_hours_log)\n",
    "players_training <- training(players_split)\n",
    "players_testing <- testing(players_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fe86b3-407b-4209-a42d-c338f9cf22c8",
   "metadata": {},
   "source": [
    "Now, we start working on creating the model. The first step is to add a recipe, where we specify the response variable, `played_hours_log`, and the predictor variables: `experience_fct`, `Age`, and `gender_fct`. Since we have factor (categorical) variables, we use `step_dummy()` to turn these variables into **dummy** numerical variables, since our model can't really understand non-numerical variables. `step_zv` is used  to check every predictor and removes columns that only have one observation and 0 variance. `step_scale()` and `step_center` is used to scale and standardized so now it has a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "Next, we create a specification where we train the classifier and specify that it's a KNN regression. We used the `tune()` function in neighbours so we can perform cross-validation using `vfold_cv()`. Having more folds gives us better estimates with less variance. Finally, we can prepare a workflow where we bring everything together and train the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73663f9-7113-4f8f-a625-0daf66855f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(1234)\n",
    "#make a recipe for preprocessing the data; scale and center the data\n",
    "players_recipe <- recipe(played_hours_log ~ experience_fct + Age + gender_fct, data = players_training) |>\n",
    "    step_dummy(all_nominal_predictors()) |>\n",
    "    step_zv(all_predictors()) |>\n",
    "    step_scale(all_numeric_predictors()) |>\n",
    "    step_center(all_numeric_predictors())\n",
    "\n",
    "#make a knn model, where the best k value is found\n",
    "players_spec <- nearest_neighbor(weight_func = \"rectangular\", neighbors = tune()) |>\n",
    "    set_engine(\"kknn\") |>\n",
    "    set_mode(\"regression\")\n",
    "\n",
    "#conduct cross validation where there are 10 folds\n",
    "players_vfold <- vfold_cv(players_training, v = 10, strata = played_hours_log)\n",
    "\n",
    "#create a workflow\n",
    "players_wkflw <- workflow() |>\n",
    "    add_recipe(players_recipe) |>\n",
    "    add_model(players_spec)\n",
    "players_wkflw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e138aabe-4d4f-4bba-ab54-d14f85c315f1",
   "metadata": {},
   "source": [
    "Next we try an array of different possible k values between 1 and 50 and add it to our classifier. To determine the K value with the lowest RMSE value, we will filter the metrics we have and pull the k value with the lowest RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3e16d9-5200-4b0f-b4cf-4c0caa18f0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(1234)\n",
    "#run cross validation and determine the best k value for the training data set\n",
    "gridvals <- tibble(neighbors = seq(from = 1, to = 50, by = 1))\n",
    "players_results <- players_wkflw |>\n",
    "    tune_grid(resamples = players_vfold, grid = gridvals) |>\n",
    "    collect_metrics() |>\n",
    "    filter(.metric == \"rmse\")\n",
    "k_min <- players_results |>\n",
    "    filter(mean == min(mean)) |>\n",
    "    pull(neighbors)\n",
    "k_min\n",
    "#k value with lowest rmspe value is k = 19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a85a8cf-5bd9-4dcf-8012-f68b62550d2f",
   "metadata": {},
   "source": [
    "To further see the relationship between the k values and their associated RMSE value, a line graph was made. Based on the graph, lower k values than that of the determined `k_min` seemed to have a higher RMSE, which could be indicative that the model would be influenced by every data point, making it overfit the data. However, if we look at the RMSE values past the `k_min` value, we see that there are not that many drastic changes and it seems to possibly plateau. This could be indicative that those k-values can also perform well to a certain degree. This could make sense because we have used 3 predictor variables, which could make the KNN more stable because our points might be relatively stable and not too far from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cdccdd-12a0-47d1-93bb-7c912ef20e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(1234)\n",
    "rmse_versus_k <- ggplot(players_results, aes(x = neighbors, y = mean)) +\n",
    "    geom_line() +\n",
    "    labs(x = \"Neighbors\", y = \"RMSE\", title = \"RMSE for 1 ≤ k ≤ 50\") +\n",
    "    scale_x_continuous(breaks = seq(from = 0, to = 50, by = 5)) +\n",
    "    theme(text = element_text(size = 15))\n",
    "rmse_versus_k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564c5049-1b63-4933-a7e9-ee9794dd8af2",
   "metadata": {},
   "source": [
    "*figure 7*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31eca7d-5ced-471f-88e7-307706c05ce8",
   "metadata": {},
   "source": [
    "Finally, we will apply the trained classifier with its optimal k value and assess how the model does with unseen data (testing data). The metrics were collected, and the RMSPE value was determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06aaf2da-93a7-4ca4-9d0d-68e143b19581",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(1234)\n",
    "#apply this k value to the testing set\n",
    "players_spec <- nearest_neighbor(weight_func = \"rectangular\", neighbors = k_min) |>\n",
    "    set_engine(\"kknn\") |>\n",
    "    set_mode(\"regression\")\n",
    "players_spec\n",
    "\n",
    "players_fit <- workflow() |>\n",
    "    add_recipe(players_recipe) |>\n",
    "    add_model(players_spec) |>\n",
    "    fit(data = players_training)\n",
    "\n",
    "players_predictions <- players_fit |>\n",
    "    predict(players_testing) |>\n",
    "    bind_cols(players_testing)\n",
    "players_predictions\n",
    "\n",
    "players_summary <- players_predictions|>\n",
    "    metrics(truth = played_hours_log, estimate = .pred) |>\n",
    "    filter(.metric == \"rmse\")\n",
    "players_summary\n",
    "\n",
    "print(glue(\"The RMSPE value using a KNN regression model is {players_summary |> pull(.estimate)}\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199f6404-dfc3-471a-a92b-e286db776036",
   "metadata": {},
   "source": [
    "*figure 8*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b255c3f3-e882-4058-9e34-1377d0ce346f",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_visualization <- ggplot(players_predictions, aes(x = .pred, y = played_hours_log)) +\n",
    "  geom_point(alpha = 0.6) +\n",
    "  geom_abline(slope = 1, intercept = 0, color = \"red\") +\n",
    "  labs(title = \"Predicted vs Actual Played Hours\",\n",
    "       x = \"Predicted Played Hours\",\n",
    "       y = \"Actual Played Hours\")+\n",
    "    theme(plot.title = element_text(size = 16),\n",
    "        axis.title = element_text(size = 14))+\n",
    "        ggtitle(\"Predicted vs Actual Played Hours (using knn regression model)\")\n",
    "knn_visualization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76704850-6637-4e85-ad49-e7c521c95177",
   "metadata": {},
   "source": [
    "*figure 9*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e792b7-4c57-47c6-b0ef-ee510848bc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_plot<- players_predictions |>\n",
    "  mutate(residual = played_hours_log - .pred) |>\n",
    "  ggplot(aes(x = .pred, y = residual)) +\n",
    "  geom_point(alpha = 0.6) +\n",
    "  geom_hline(yintercept = 0, color = \"red\") +\n",
    "  labs(\n",
    "    x = \"Predicted (log played hours)\",\n",
    "    y = \"Residual\",\n",
    "    title = \"Residual Plot\"\n",
    "  ) +\n",
    "  theme_minimal()\n",
    "\n",
    "residual_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e0e432-e7a8-4ec3-a65b-e0212c46e98e",
   "metadata": {},
   "source": [
    "The residual plot shows a curved pattern which indicates that a simple knn regression model does not fully capture the relationship between the predictors (age, experience and gender) and log transformed played hours. This suggests potential non-linearity and the presence of outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493006bb-666f-4e86-8b7f-fc07cff567da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(players_predictions, n = Inf, width = Inf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031bf85d-8758-4e43-ad09-d5937e6e8bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidymodels)\n",
    "\n",
    "engine_fit <- extract_fit_engine(players_fit)\n",
    "engine_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d457031-82e0-4ec9-a2cf-1bdfde37a723",
   "metadata": {},
   "source": [
    "# Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd2fe92-b50c-44e2-9908-cd61d010b11e",
   "metadata": {},
   "source": [
    "### 1. Comparing .pred and players_hours_log ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76920094-ffb5-458b-86d1-dfa11771a563",
   "metadata": {},
   "source": [
    "|Category | Predicted values | Actual | Interpretation |\n",
    "|---------| -----| ------------| ---------------|\n",
    "|Regular| ~1.03–1.31|  0.095 → 0.916 |Predictions are consistently high, but actual values are moderate or low and KNN tends to overpredict Regulars.| \n",
    "|Amateur | ~0.81–1.11| 0.09 → 4.01 | Amateurs receive mid-to-high predictions as many are close in prediction (e.g., 0.848 vs 0.642), but some are way off (e.g., 0.848 predicted vs 4.01 actual).|\n",
    "|Veteran| 0.33–0.58 | 0.095 → 1.57| Veterans mostly get low predictios with KNN underpredicting high-hour Veterans |\n",
    "|Beginner| ~0.58–0.62 |range from very low (0.0953) to very high (3.21)|KNN predicts consistently low hours for Beginners. Actual hours vary widely → model underpredicts high-hour Beginners.| \n",
    "|Pro| ~0.52–0.82 | 0.182 → 3.44 | Predictions are mid-range but some actual values are extremely high: ow 1: actual = 3.44 → huge underprediction |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf046b3-4d43-4e3a-b4a3-f654c4bdadd7",
   "metadata": {},
   "source": [
    "|Category | Predicted values | Actual | Interpretation |\n",
    "|---------| -----| ------------| ---------------|\n",
    "|Regular| ~1.03–1.31|  0.095 → 0.916 |Predictions are consistently high, but actual values are moderate or low and KNN tends to overpredict Regulars.| \n",
    "|Amateur | ~0.81–1.11| 0.09 → 4.01 | Amateurs receive mid-to-high predictions as many are close in prediction (e.g., 0.848 vs 0.642), but some are way off (e.g., 0.848 predicted vs 4.01 actual).|\n",
    "|Veteran| 0.33–0.58 | 0.095 → 1.57| Veterans mostly get low predictios with KNN underpredicting high-hour Veterans |\n",
    "|Beginner| ~0.58–0.62 |range from very low (0.0953) to very high (3.21)|KNN predicts consistently low hours for Beginners. Actual hours vary widely → model underpredicts high-hour Beginners.| \n",
    "|Pro| ~0.52–0.82 | 0.182 → 3.44 | Predictions are mid-range but some actual values are extremely high: ow 1: actual = 3.44 → huge underprediction |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0201bf-0630-47df-9021-32e997e839b8",
   "metadata": {},
   "source": [
    "### Summary of Results ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55a9baf-50d2-4fca-92bd-2f68bed2ac27",
   "metadata": {},
   "source": [
    "**Regular players** playing more hours fits expectations—they’re the most engaged but not yet burned out like some Veterans.\n",
    "\n",
    "\n",
    "**Age** having almost no effect is expected because age alone doesn’t strongly predict gaming behavior.\n",
    "\n",
    "\n",
    "**Gender** differences can exist, but the strong positive effect for Agender individuals may be unexpected. However, this could be due to:\n",
    "\n",
    "-small sample size\n",
    "\n",
    "-high variability within groups\n",
    "\n",
    "-unique gaming motivations in minority gender groups\n",
    "\n",
    "\n",
    "So some results are intuitive, while others (like Agender being the strongest) may reflect your dataset’s specific composition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a1b23c-8864-49a8-827d-54c886182c6a",
   "metadata": {},
   "source": [
    "# Bibliography:\n",
    "\n",
    "Rehbein, F., Staudt, A., Hanslmaier, M., & Kliem, S. (2016). Video game playing in the general adult population of Germany: Can higher gaming time of males be explained by gender specific genre preferences? Computers in Human Behavior, 55, 729–735. https://doi.org/10.1016/j.chb.2015.10.016 \n",
    "\n",
    "Bunz, U., Cortese, J., & Sellers, N. (2020). Examining younger and older adults’ digital gaming habits and health measures. Gerontechnology, 19(4), 1–10. https://doi.org/10.4017/gt.2020.19.04.381\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
